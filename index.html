<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shruvaan: The First Secure Multilingual Model Context Protocol (MCP) for Natural Language Communication Among LLM Agents</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <div class="container">
        <!-- Header Section -->
        <header class="header">
            <div class="title-section">
                <div class="title-content">
                    <h1 class="main-title">The First Secure Multilingual Model Context Protocol (MCP) for Natural Language Communication Among LLM Agents</h1>
                </div>
            </div>

            <div class="authors-section">
                <p class="authors">
                    <span class="author"></span><sup class="contrib">*</sup>, 
                    <span class="author"></span><sup class="contrib">*</sup>, 
                    <span class="author"></span><sup class="contrib">*</sup>, 
                    <span class="author"></span><sup class="contrib">*</sup>, 
                    <span class="author"></span>
                </p>
                <p class="affiliation">AI Institute, University of South Carolina, USA</p>
                <p class="equal-contribution">* equal contribution</p>
            </div>

            <div class="action-buttons">
                <button class="btn btn-paper" id="paperBtn">
                    <i class="fas fa-file-alt"></i> Paper
                </button>
                <button class="btn btn-code" id="codeBtn">
                    <i class="fas fa-code"></i> Code
                </button>
            </div>
        </header>

        <!-- Technical Diagram Section -->
        <section class="diagram-section">
            <div class="diagram-container">
                <div class="latent-space-header">
                    <h3>Latent Space</h3>
                </div>
                
                <div class="main-image-placeholder">
                    <i class="fas fa-image"></i>
                    <h4>Image Placeholder</h4>
                    <p>Replace this section with your technical diagram or image</p>
                    <div class="image-instructions">
                        <small>
                            To add your image:<br>
                            1. Save your image in the <code>images/</code> folder<br>
                            2. Replace this placeholder with: <code>&lt;img src="images/your-image.png" alt="Your Description" /&gt;</code>
                        </small>
                    </div>
                </div>
            </div>
        </section>

        <!-- Abstract Section -->
        <section class="abstract-section">
            <h2 class="section-title">Abstract</h2>
            <div class="abstract-content">
                <p>
                    Alignment is crucial for text-to-image (T2I) models to ensure that the generated images faithfully capture user intent while maintaining safety and fairness. <strong>Direct Preference Optimization (DPO)</strong> has emerged as a key alignment technique for large language models (LLMs), and its influence is now extending to T2I systems. This paper introduces <strong>DPO-Kernels for T2I models</strong>, a novel extension of DPO that enhances alignment across three key dimensions: (i) <strong>Hybrid Loss</strong>, which integrates embedding-based objectives with the traditional probability-based loss to improve optimization; (ii) <strong>Kernelized Representations</strong>, leveraging <strong>Radial Basis Function (RBF)</strong>, <strong>Polynomial</strong>, and <strong>Wavelet</strong> kernels to enable richer feature transformations, ensuring better separation between safe and unsafe inputs; and (iii) <strong>Divergence Selection</strong>, expanding beyond DPO's default <strong>Kullback-Leibler (KL)</strong> regularizer by incorporating alternative divergence measures such as <strong>Wasserstein</strong> and <strong>Renyi</strong> divergences to enhance stability and robustness in alignment training as shown in Fig. 1.
                </p>
                <p>
                    We introduce <strong>DETONATE</strong>, the first large-scale benchmark of its kind, comprising approximately 100K curated image pairs, categorized as <em>chosen</em> and <em>rejected</em>. This benchmark encapsulates three critical axes of social bias and discrimination: <strong>Race</strong>, <strong>Gender</strong>, and <strong>Disability</strong>. The prompts are sourced from the hate speech datasets, while the images are generated using state-of-the-art T2I models, including Stable Diffusion 3.5 Large (SD-3.5), Stable Diffusion XL (SD-XL), and Midjourney. Furthermore, to evaluate alignment beyond surface metrics, we introduce the <strong>Alignment Quality Index (AQI)</strong>, a novel geometric measure that quantifies latent space separability of safe/unsafe image activations, revealing hidden model vulnerabilities. While alignment techniques often risk overfitting, we empirically demonstrate that <strong>DPO-Kernels</strong> preserve strong generalization bounds using the theory of <strong>Heavy-tailed Self-Regularization (HT-SR)</strong>.
                </p>
            </div>
        </section>
    </div>

    <script src="js/script.js"></script>
</body>
</html>
